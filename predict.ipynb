{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import imblearn\n",
    "import sklearn.inspection\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in manga dataframe from CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('manga.csv', na_values = ['','nan','None'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data wrangling** \n",
    "\n",
    "Replace NaNs with zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_1 = df_original\n",
    "updated_df_1[\"chapters\"] = df_original[\"chapters\"].fillna(value=-1)\n",
    "updated_df_1[\"volumes\"] = updated_df_1[\"volumes\"].fillna(value=-1)\n",
    "\n",
    "\n",
    "numeric_columns = df_original.select_dtypes(include=['number']).columns\n",
    "\n",
    "updated_df_1 = df_original\n",
    "updated_df_1[numeric_columns] = updated_df_1[numeric_columns].fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding single column indicating which demographic the manga belongs to and deleting separate columns specifying this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3627/1673362650.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  updated_df_2[\"Demographic\"] = demo_column_2\n"
     ]
    }
   ],
   "source": [
    "demographic_labels = [\"Shounen\",\"Shoujo\",\"Seinen\",\"Josei\"]\n",
    "demo_only_df = updated_df_1.loc[:,demographic_labels]\n",
    "\n",
    "# Finding instances with no clear demographic tag\n",
    "total_pct_per_row = demo_only_df.sum(axis=1).tolist()\n",
    "indices = [index for index, element in enumerate(total_pct_per_row) if element == 0]\n",
    "\n",
    "# Finding best fit for each row, aside from no-tag cases\n",
    "demo_column_1 = demo_only_df.idxmax(axis=1).tolist()\n",
    "demo_column_2 = demo_column_1\n",
    "\n",
    "# Replacing best fit demographic with \"Unknown\" in cases with no clear demographic tag\n",
    "for index in indices:\n",
    "    demo_column_2[index] = \"Unknown\"\n",
    "\n",
    "# Adding column\n",
    "updated_df_2 = updated_df_1\n",
    "updated_df_2[\"Demographic\"] = demo_column_2\n",
    "\n",
    "# Removing demographic tag columns\n",
    "updated_df_3 = updated_df_2.drop(demographic_labels, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating popularity column by summing all status columns, recalculating status raw numbers to percentage of overall statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting current list of columns\n",
    "columns_list_v3 = updated_df_3.columns.tolist()\n",
    "\n",
    "# Creating list of status columns only\n",
    "status_regex = re.compile(\"status_\")\n",
    "status_raws_cols = list(filter(status_regex.match, columns_list_v3))\n",
    "\n",
    "# Summing status columns only\n",
    "popularity_list = updated_df_3[status_raws_cols].sum(axis=1).tolist()\n",
    "\n",
    "# Adding new column\n",
    "updated_df_4 = updated_df_3\n",
    "updated_df_4[\"Popularity\"] = popularity_list\n",
    "\n",
    "# Recalculating statuses to be a percentage of popularity\n",
    "for column in status_raws_cols:\n",
    "    updated_df_4 = updated_df_4.assign(**{column: updated_df_4[column]/updated_df_4[\"Popularity\"]})\n",
    "\n",
    "# Renaming status columns to reflect their new meaning (this also renames score columns, which will be updated later)\n",
    "updated_df_4.columns = updated_df_4.columns.str.replace(\"_count\", \"_pct\", regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating total number of ratings across each manga, converting the count from each score bin into a percentage of the total ratings, and then transforming the total number of ratings into a percentage of the total popularity of the manga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting current list of columns\n",
    "columns_list_v4 = updated_df_4.columns.tolist()\n",
    "\n",
    "# Creating list of score columns only\n",
    "score_regex = re.compile(\"scored_\")\n",
    "score_raws_cols = list(filter(score_regex.match, columns_list_v4))\n",
    "\n",
    "# Summing score columns only\n",
    "score_list = updated_df_4[score_raws_cols].sum(axis=1).tolist()\n",
    "\n",
    "# Adding new column\n",
    "updated_df_5 = updated_df_4\n",
    "updated_df_5[\"Scored_Percentage\"] = score_list\n",
    "\n",
    "# Recalculating statuses to be a percentage of popularity\n",
    "for column in score_raws_cols:\n",
    "    updated_df_5 = updated_df_5.assign(**{column: updated_df_5[column]/updated_df_5[\"Scored_Percentage\"]})\n",
    "\n",
    "updated_df_5 = updated_df_5.assign(Scored_Percentage = updated_df_5[\"Scored_Percentage\"]/updated_df_5[\"Popularity\"])\n",
    "\n",
    "numeric_columns = updated_df_5.select_dtypes(include=['number']).columns\n",
    "updated_df_5[numeric_columns] = updated_df_5[numeric_columns].fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming favorites column into a percentage of popularity as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_6 = updated_df_5.assign(favorites = updated_df_5[\"favorites\"]/updated_df_5[\"Popularity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating run length for completed works, filling -1 for ongoing works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_7 = updated_df_6.assign(run_length = updated_df_6[\"end_date_days\"]-updated_df_6[\"start_date_days\"])\n",
    "updated_df_7[\"run_length\"] = updated_df_7[\"run_length\"].where(cond=updated_df_7[\"run_length\"]>-1, other=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding for categorical variables; removing unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_8 = updated_df_7.drop([\"id\", \"eng_title\", \"rom_title\",\"start_date\",\"end_date\"], axis=1)\n",
    "updated_df_8 = pd.get_dummies(updated_df_8, dummy_na=True, columns=[\"status\",\"source\",\"country\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building First Classifier**\n",
    "\n",
    "Splitting into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = updated_df_8.query(\"Demographic != 'Unknown'\")\n",
    "training_x = training_df.drop(columns=[\"Demographic\"])\n",
    "training_y = training_df[[\"Demographic\"]]\n",
    "testing_df = updated_df_8.query(\"Demographic == 'Unknown'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and fitting balanced random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zach/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BalancedRandomForestClassifier(bootstrap=True, n_estimators=500, oob_score=True,\n",
       "                               random_state=1234, replacement=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfBaseline = imblearn.ensemble.BalancedRandomForestClassifier(n_estimators=500, oob_score = True, replacement=False, bootstrap=True, random_state=1234)\n",
    "\n",
    "rfBaseline.fit(training_x, np.ravel(training_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting OOB classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Shounen': {'precision': 0.5957152729785764,\n",
       "  'recall': 0.5910181693520741,\n",
       "  'f1-score': 0.5933574255721906,\n",
       "  'support': 2917},\n",
       " 'Shoujo': {'precision': 0.7133182844243793,\n",
       "  'recall': 0.657511444028298,\n",
       "  'f1-score': 0.6842789086184495,\n",
       "  'support': 2403},\n",
       " 'Seinen': {'precision': 0.6035332785538209,\n",
       "  'recall': 0.4469120778825677,\n",
       "  'f1-score': 0.5135465827652508,\n",
       "  'support': 3287},\n",
       " 'Josei': {'precision': 0.2745205479452055,\n",
       "  'recall': 0.6583442838370565,\n",
       "  'f1-score': 0.3874709976798144,\n",
       "  'support': 761},\n",
       " 'accuracy': 0.5629803586678053,\n",
       " 'macro avg': {'precision': 0.5467718459754956,\n",
       "  'recall': 0.588446493774999,\n",
       "  'f1-score': 0.5446634786589264,\n",
       "  'support': 9368},\n",
       " 'weighted avg': {'precision': 0.6025330179700044,\n",
       "  'recall': 0.5629803586678053,\n",
       "  'f1-score': 0.5719512035213421,\n",
       "  'support': 9368}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classorder = rfBaseline.classes_\n",
    "OOBdecisionfunc = rfBaseline.oob_decision_function_\n",
    "\n",
    "OOBPreds = pd.DataFrame(data=OOBdecisionfunc, columns=classorder).idxmax(axis=1).tolist()\n",
    "\n",
    "sklearn.metrics.classification_report(training_y, OOBPreds, labels=[\"Shounen\",\"Shoujo\", \"Seinen\", \"Josei\"], output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our macro average recall is 59%, which is markedly higher than the roughly 25% we would see if the classifier was guessing at random.\n",
    "\n",
    "**Refining Model**\n",
    "\n",
    "Let's start by checking pairwise correlation coefficients between each set of our predictors, and removing those which are more than 80% correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = training_x.corr().stack().reset_index()\n",
    "temp_2 = temp.rename(columns={\"level_0\":\"first_var\", \"level_1\":\"second_var\", 0:\"correlation\"}, inplace=False)\n",
    "temp_2[\"correlation\"] = temp_2[\"correlation\"].abs()\n",
    "\n",
    "temp_3 = temp_2.query(\"first_var != second_var\").sort_values(\"correlation\", ascending=False)\n",
    "\n",
    "remvar = []\n",
    "temp_vals = training_x\n",
    "\n",
    "while temp_3.iloc[0,2] >= .8:\n",
    "    remvar.append(temp_3.iloc[0,0])\n",
    "    temp_vals = temp_vals.drop(remvar[-1], axis=1)\n",
    "    temp_3 = temp_vals.corr().stack().reset_index().rename(columns={\"level_0\":\"first_var\", \"level_1\":\"second_var\", 0:\"correlation\"}).query(\"first_var != second_var\")\n",
    "    temp_3[\"correlation\"] = temp_3[\"correlation\"].abs()\n",
    "    temp_3 = temp_3.sort_values(\"correlation\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a randomized feature to use for variable selection so that we can determine which variables to remove in order to improve speed without sacrificing too much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' train_w_random = temp_vals\\ntrain_w_random[\"RANDOM\"] = np.random.RandomState(1234).randn(train_w_random.shape[0])\\n\\nfeatselectrf = imblearn.ensemble.BalancedRandomForestClassifier(n_estimators=100, oob_score = True, replacement=False, bootstrap=True, random_state=1234)\\nfeatselectrf.fit(train_w_random, np.ravel(training_y))\\n\\nvarimp_forselection = pd.DataFrame({\"names\": train_w_random.columns, \"imp\": featselectrf.feature_importances_}).sort_values(\"imp\", ascending=True)\\nfor_removal = []\\n\\nwhile varimp_forselection[varimp_forselection.imp < varimp_forselection.query(\"names == \\'RANDOM\\'\").iloc[0,1]].shape[0] > 0:\\n    if varimp_forselection[varimp_forselection.imp < varimp_forselection.query(\"names == \\'RANDOM\\'\").iloc[0,1]].shape[0] >= 25:\\n        for_removal = varimp_forselection.iloc[0:25,0]\\n    else:\\n        num_remove = varimp_forselection[varimp_forselection.imp < varimp_forselection.query(\"names == \\'RANDOM\\'\").iloc[0,1]].shape[0]\\n        for_removal = varimp_forselection.iloc[:num_remove,0]\\n    train_w_random = train_w_random.drop(for_removal, axis=1)\\n    featselectrf.fit(train_w_random, np.ravel(training_y))\\n    varimp_forselection = pd.DataFrame({\"names\": train_w_random.columns, \"imp\": featselectrf.feature_importances_}).sort_values(\"imp\", ascending=True)\\n\\nprint(\"remaining columns\"+train_w_random.columns) '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" train_w_random = temp_vals\n",
    "train_w_random[\"RANDOM\"] = np.random.RandomState(1234).randn(train_w_random.shape[0])\n",
    "\n",
    "featselectrf = imblearn.ensemble.BalancedRandomForestClassifier(n_estimators=100, oob_score = True, replacement=False, bootstrap=True, random_state=1234)\n",
    "featselectrf.fit(train_w_random, np.ravel(training_y))\n",
    "\n",
    "varimp_forselection = pd.DataFrame({\"names\": train_w_random.columns, \"imp\": featselectrf.feature_importances_}).sort_values(\"imp\", ascending=True)\n",
    "for_removal = []\n",
    "\n",
    "while varimp_forselection[varimp_forselection.imp < varimp_forselection.query(\"names == 'RANDOM'\").iloc[0,1]].shape[0] > 0:\n",
    "    if varimp_forselection[varimp_forselection.imp < varimp_forselection.query(\"names == 'RANDOM'\").iloc[0,1]].shape[0] >= 25:\n",
    "        for_removal = varimp_forselection.iloc[0:25,0]\n",
    "    else:\n",
    "        num_remove = varimp_forselection[varimp_forselection.imp < varimp_forselection.query(\"names == 'RANDOM'\").iloc[0,1]].shape[0]\n",
    "        for_removal = varimp_forselection.iloc[:num_remove,0]\n",
    "    train_w_random = train_w_random.drop(for_removal, axis=1)\n",
    "    featselectrf.fit(train_w_random, np.ravel(training_y))\n",
    "    varimp_forselection = pd.DataFrame({\"names\": train_w_random.columns, \"imp\": featselectrf.feature_importances_}).sort_values(\"imp\", ascending=True)\n",
    "\n",
    "print(\"remaining columns\"+train_w_random.columns) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train and validate a model based on the reamining 12 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' train_w_random_1 = train_w_random.drop(\"RANDOM\", axis=1)\\n\\nrfReduced = imblearn.ensemble.BalancedRandomForestClassifier(n_estimators=500, oob_score = True, replacement=False, bootstrap=True, random_state=1234)\\n\\nrfReduced.fit(train_w_random_1, np.ravel(training_y))\\n\\nclassorder = rfReduced.classes_\\nOOBdecisionfunc = rfReduced.oob_decision_function_\\n\\nOOBPreds = pd.DataFrame(data=OOBdecisionfunc, columns=classorder).idxmax(axis=1).tolist()\\n\\nsklearn.metrics.classification_report(training_y, OOBPreds, labels=[\"Shounen\",\"Shoujo\", \"Seinen\", \"Josei\"], output_dict=True) '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" train_w_random_1 = train_w_random.drop(\"RANDOM\", axis=1)\n",
    "\n",
    "rfReduced = imblearn.ensemble.BalancedRandomForestClassifier(n_estimators=500, oob_score = True, replacement=False, bootstrap=True, random_state=1234)\n",
    "\n",
    "rfReduced.fit(train_w_random_1, np.ravel(training_y))\n",
    "\n",
    "classorder = rfReduced.classes_\n",
    "OOBdecisionfunc = rfReduced.oob_decision_function_\n",
    "\n",
    "OOBPreds = pd.DataFrame(data=OOBdecisionfunc, columns=classorder).idxmax(axis=1).tolist()\n",
    "\n",
    "sklearn.metrics.classification_report(training_y, OOBPreds, labels=[\"Shounen\",\"Shoujo\", \"Seinen\", \"Josei\"], output_dict=True) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that didn't work at all, likely because the random variable had much higher cardinality than many other variables. Let's try permutation-based importance scores instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zach/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "rfPermutation = imblearn.ensemble.BalancedRandomForestClassifier(n_estimators=100, oob_score = True, replacement=False, bootstrap=True, random_state=1234)\n",
    "rfPermutation.fit(temp_vals, np.ravel(training_y))\n",
    "\n",
    "permute_results = sklearn.inspection.permutation_importance(estimator=rfPermutation, X = temp_vals, y = training_y, scoring=\"recall_macro\", n_repeats=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_importance = pd.DataFrame()\n",
    "perm_importance[\"feature\"] = temp_vals.columns\n",
    "perm_importance[\"importance\"] = permute_results[\"importances_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try removing any variable that isn't associated with an average of at least a 1/20th of a percent increase in macro averaged recall. This is somewhat arbitrary, but will hopefully result in a much more efficient random forest with a negligible decrease in recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zach/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Shounen': {'precision': 0.5914848698099929,\n",
       "  'recall': 0.5762769969146383,\n",
       "  'f1-score': 0.5837819065810036,\n",
       "  'support': 2917},\n",
       " 'Shoujo': {'precision': 0.7040540540540541,\n",
       "  'recall': 0.6504369538077404,\n",
       "  'f1-score': 0.6761842959117457,\n",
       "  'support': 2403},\n",
       " 'Seinen': {'precision': 0.5955940204563336,\n",
       "  'recall': 0.4606023729844843,\n",
       "  'f1-score': 0.5194716074798421,\n",
       "  'support': 3287},\n",
       " 'Josei': {'precision': 0.2687074829931973,\n",
       "  'recall': 0.6228646517739816,\n",
       "  'f1-score': 0.37544554455445545,\n",
       "  'support': 761},\n",
       " 'accuracy': 0.5584970111016225,\n",
       " 'macro avg': {'precision': 0.5399601068283946,\n",
       "  'recall': 0.5775452438702112,\n",
       "  'f1-score': 0.5387208386317617,\n",
       "  'support': 9368},\n",
       " 'weighted avg': {'precision': 0.5955814685018609,\n",
       "  'recall': 0.5584970111016225,\n",
       "  'f1-score': 0.5679952943813935,\n",
       "  'support': 9368}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm_features_tokeep = perm_importance.query(\"importance >= .0005\").feature\n",
    "\n",
    "refined_training = temp_vals[perm_features_tokeep]\n",
    "\n",
    "rfReduced2 = imblearn.ensemble.BalancedRandomForestClassifier(n_estimators=500, oob_score = True, replacement=False, bootstrap=True, random_state=1234)\n",
    "\n",
    "rfReduced2.fit(refined_training, np.ravel(training_y))\n",
    "\n",
    "classorder = rfReduced2.classes_\n",
    "OOBdecisionfunc = rfReduced2.oob_decision_function_\n",
    "\n",
    "OOBPreds = pd.DataFrame(data=OOBdecisionfunc, columns=classorder).idxmax(axis=1).tolist()\n",
    "\n",
    "sklearn.metrics.classification_report(training_y, OOBPreds, labels=[\"Shounen\",\"Shoujo\", \"Seinen\", \"Josei\"], output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macro average recall decreased by about 1%; next we can tune hyperparameters in order to improve our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=3, n_estimators=1300, random_state=1234, replacement=True, sampling_strategy=all; total time=   7.7s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=3, n_estimators=1300, random_state=1234, replacement=True, sampling_strategy=all; total time=   7.4s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=3, n_estimators=1300, random_state=1234, replacement=True, sampling_strategy=all; total time=   7.4s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=gini, max_depth=30, min_samples_leaf=1, min_samples_split=3, n_estimators=200, random_state=1234, replacement=True, sampling_strategy=not majority; total time=   2.5s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=gini, max_depth=30, min_samples_leaf=1, min_samples_split=3, n_estimators=200, random_state=1234, replacement=True, sampling_strategy=not majority; total time=   2.4s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=gini, max_depth=30, min_samples_leaf=1, min_samples_split=3, n_estimators=200, random_state=1234, replacement=True, sampling_strategy=not majority; total time=   2.3s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=gini, max_depth=25, min_samples_leaf=4, min_samples_split=7, n_estimators=800, random_state=1234, replacement=True, sampling_strategy=majority; total time=  10.3s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=gini, max_depth=25, min_samples_leaf=4, min_samples_split=7, n_estimators=800, random_state=1234, replacement=True, sampling_strategy=majority; total time=  10.0s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=gini, max_depth=25, min_samples_leaf=4, min_samples_split=7, n_estimators=800, random_state=1234, replacement=True, sampling_strategy=majority; total time=   9.5s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=gini, max_depth=20, min_samples_leaf=1, min_samples_split=7, n_estimators=800, random_state=1234, replacement=True, sampling_strategy=all; total time=   5.7s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=gini, max_depth=20, min_samples_leaf=1, min_samples_split=7, n_estimators=800, random_state=1234, replacement=True, sampling_strategy=all; total time=   5.8s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=gini, max_depth=20, min_samples_leaf=1, min_samples_split=7, n_estimators=800, random_state=1234, replacement=True, sampling_strategy=all; total time=   5.4s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=gini, max_depth=4, min_samples_leaf=2, min_samples_split=15, n_estimators=1300, random_state=1234, replacement=True, sampling_strategy=not minority; total time=   5.3s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=gini, max_depth=4, min_samples_leaf=2, min_samples_split=15, n_estimators=1300, random_state=1234, replacement=True, sampling_strategy=not minority; total time=   5.1s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=gini, max_depth=4, min_samples_leaf=2, min_samples_split=15, n_estimators=1300, random_state=1234, replacement=True, sampling_strategy=not minority; total time=   5.0s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=1800, random_state=1234, replacement=True, sampling_strategy=all; total time=   7.5s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=1800, random_state=1234, replacement=True, sampling_strategy=all; total time=   7.4s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=1800, random_state=1234, replacement=True, sampling_strategy=all; total time=   7.3s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=gini, max_depth=40, min_samples_leaf=4, min_samples_split=3, n_estimators=500, random_state=1234, replacement=True, sampling_strategy=not minority; total time=   3.3s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=gini, max_depth=40, min_samples_leaf=4, min_samples_split=3, n_estimators=500, random_state=1234, replacement=True, sampling_strategy=not minority; total time=   3.3s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=gini, max_depth=40, min_samples_leaf=4, min_samples_split=3, n_estimators=500, random_state=1234, replacement=True, sampling_strategy=not minority; total time=   3.1s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=entropy, max_depth=45, min_samples_leaf=2, min_samples_split=10, n_estimators=2000, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  28.6s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=entropy, max_depth=45, min_samples_leaf=2, min_samples_split=10, n_estimators=2000, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  27.6s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=entropy, max_depth=45, min_samples_leaf=2, min_samples_split=10, n_estimators=2000, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  27.1s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=gini, max_depth=35, min_samples_leaf=1, min_samples_split=7, n_estimators=2500, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  30.6s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=gini, max_depth=35, min_samples_leaf=1, min_samples_split=7, n_estimators=2500, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  28.9s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=gini, max_depth=35, min_samples_leaf=1, min_samples_split=7, n_estimators=2500, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  28.2s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=gini, max_depth=25, min_samples_leaf=5, min_samples_split=5, n_estimators=2500, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  26.5s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=gini, max_depth=25, min_samples_leaf=5, min_samples_split=5, n_estimators=2500, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  25.9s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=gini, max_depth=25, min_samples_leaf=5, min_samples_split=5, n_estimators=2500, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  25.3s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=7, n_estimators=800, random_state=1234, replacement=True, sampling_strategy=majority; total time=  14.8s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=7, n_estimators=800, random_state=1234, replacement=True, sampling_strategy=majority; total time=  14.4s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=7, n_estimators=800, random_state=1234, replacement=True, sampling_strategy=majority; total time=  13.7s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=gini, max_depth=30, min_samples_leaf=5, min_samples_split=15, n_estimators=2500, random_state=1234, replacement=True, sampling_strategy=majority; total time=  31.1s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=gini, max_depth=30, min_samples_leaf=5, min_samples_split=15, n_estimators=2500, random_state=1234, replacement=True, sampling_strategy=majority; total time=  30.3s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=gini, max_depth=30, min_samples_leaf=5, min_samples_split=15, n_estimators=2500, random_state=1234, replacement=True, sampling_strategy=majority; total time=  31.1s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=25, min_samples_leaf=1, min_samples_split=2, n_estimators=2000, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  32.7s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=25, min_samples_leaf=1, min_samples_split=2, n_estimators=2000, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  30.4s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=25, min_samples_leaf=1, min_samples_split=2, n_estimators=2000, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  28.9s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=55, min_samples_leaf=2, min_samples_split=7, n_estimators=1800, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  26.1s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=55, min_samples_leaf=2, min_samples_split=7, n_estimators=1800, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  25.3s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=55, min_samples_leaf=2, min_samples_split=7, n_estimators=1800, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  24.1s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=55, min_samples_leaf=5, min_samples_split=15, n_estimators=1300, random_state=1234, replacement=True, sampling_strategy=majority; total time=  21.0s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=55, min_samples_leaf=5, min_samples_split=15, n_estimators=1300, random_state=1234, replacement=True, sampling_strategy=majority; total time=  20.2s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=55, min_samples_leaf=5, min_samples_split=15, n_estimators=1300, random_state=1234, replacement=True, sampling_strategy=majority; total time=  19.2s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=20, min_samples_leaf=4, min_samples_split=7, n_estimators=2500, random_state=1234, replacement=True, sampling_strategy=majority; total time=  42.6s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=20, min_samples_leaf=4, min_samples_split=7, n_estimators=2500, random_state=1234, replacement=True, sampling_strategy=majority; total time=  40.7s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=20, min_samples_leaf=4, min_samples_split=7, n_estimators=2500, random_state=1234, replacement=True, sampling_strategy=majority; total time=  38.9s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=entropy, max_depth=40, min_samples_leaf=1, min_samples_split=5, n_estimators=1800, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  27.5s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=entropy, max_depth=40, min_samples_leaf=1, min_samples_split=5, n_estimators=1800, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  27.6s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=entropy, max_depth=40, min_samples_leaf=1, min_samples_split=5, n_estimators=1800, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  24.8s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=15, n_estimators=2500, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  32.6s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=15, n_estimators=2500, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  31.8s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=15, n_estimators=2500, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  30.8s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=gini, max_depth=40, min_samples_leaf=2, min_samples_split=15, n_estimators=800, random_state=1234, replacement=True, sampling_strategy=all; total time=   5.4s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=gini, max_depth=40, min_samples_leaf=2, min_samples_split=15, n_estimators=800, random_state=1234, replacement=True, sampling_strategy=all; total time=   5.3s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=gini, max_depth=40, min_samples_leaf=2, min_samples_split=15, n_estimators=800, random_state=1234, replacement=True, sampling_strategy=all; total time=   5.3s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=1300, random_state=1234, replacement=True, sampling_strategy=majority; total time=  13.5s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=1300, random_state=1234, replacement=True, sampling_strategy=majority; total time=  13.2s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=1300, random_state=1234, replacement=True, sampling_strategy=majority; total time=  12.5s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=entropy, max_depth=35, min_samples_leaf=5, min_samples_split=4, n_estimators=2000, random_state=1234, replacement=True, sampling_strategy=all; total time=  15.9s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=entropy, max_depth=35, min_samples_leaf=5, min_samples_split=4, n_estimators=2000, random_state=1234, replacement=True, sampling_strategy=all; total time=  15.2s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=entropy, max_depth=35, min_samples_leaf=5, min_samples_split=4, n_estimators=2000, random_state=1234, replacement=True, sampling_strategy=all; total time=  14.9s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=entropy, max_depth=35, min_samples_leaf=1, min_samples_split=4, n_estimators=800, random_state=1234, replacement=True, sampling_strategy=majority; total time=  15.5s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=entropy, max_depth=35, min_samples_leaf=1, min_samples_split=4, n_estimators=800, random_state=1234, replacement=True, sampling_strategy=majority; total time=  15.0s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=entropy, max_depth=35, min_samples_leaf=1, min_samples_split=4, n_estimators=800, random_state=1234, replacement=True, sampling_strategy=majority; total time=  14.2s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=60, min_samples_leaf=2, min_samples_split=15, n_estimators=200, random_state=1234, replacement=True, sampling_strategy=not minority; total time=   1.7s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=60, min_samples_leaf=2, min_samples_split=15, n_estimators=200, random_state=1234, replacement=True, sampling_strategy=not minority; total time=   1.7s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=60, min_samples_leaf=2, min_samples_split=15, n_estimators=200, random_state=1234, replacement=True, sampling_strategy=not minority; total time=   1.6s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=45, min_samples_leaf=5, min_samples_split=2, n_estimators=200, random_state=1234, replacement=True, sampling_strategy=not majority; total time=   2.8s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=45, min_samples_leaf=5, min_samples_split=2, n_estimators=200, random_state=1234, replacement=True, sampling_strategy=not majority; total time=   2.6s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=45, min_samples_leaf=5, min_samples_split=2, n_estimators=200, random_state=1234, replacement=True, sampling_strategy=not majority; total time=   2.5s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=entropy, max_depth=45, min_samples_leaf=4, min_samples_split=3, n_estimators=2000, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  27.7s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=entropy, max_depth=45, min_samples_leaf=4, min_samples_split=3, n_estimators=2000, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  26.9s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=entropy, max_depth=45, min_samples_leaf=4, min_samples_split=3, n_estimators=2000, random_state=1234, replacement=True, sampling_strategy=not majority; total time=  25.7s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=40, min_samples_leaf=2, min_samples_split=7, n_estimators=200, random_state=1234, replacement=True, sampling_strategy=all; total time=   1.8s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=40, min_samples_leaf=2, min_samples_split=7, n_estimators=200, random_state=1234, replacement=True, sampling_strategy=all; total time=   1.8s\n",
      "[CV] END bootstrap=False, class_weight=balanced_subsample, criterion=entropy, max_depth=40, min_samples_leaf=2, min_samples_split=7, n_estimators=200, random_state=1234, replacement=True, sampling_strategy=all; total time=   1.7s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=entropy, max_depth=45, min_samples_leaf=2, min_samples_split=10, n_estimators=500, random_state=1234, replacement=True, sampling_strategy=all; total time=   4.3s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=entropy, max_depth=45, min_samples_leaf=2, min_samples_split=10, n_estimators=500, random_state=1234, replacement=True, sampling_strategy=all; total time=   4.4s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=entropy, max_depth=45, min_samples_leaf=2, min_samples_split=10, n_estimators=500, random_state=1234, replacement=True, sampling_strategy=all; total time=   4.1s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=gini, max_depth=60, min_samples_leaf=1, min_samples_split=7, n_estimators=100, random_state=1234, replacement=True, sampling_strategy=majority; total time=   1.6s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=gini, max_depth=60, min_samples_leaf=1, min_samples_split=7, n_estimators=100, random_state=1234, replacement=True, sampling_strategy=majority; total time=   1.5s\n",
      "[CV] END bootstrap=False, class_weight=balanced, criterion=gini, max_depth=60, min_samples_leaf=1, min_samples_split=7, n_estimators=100, random_state=1234, replacement=True, sampling_strategy=majority; total time=   1.4s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=entropy, max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=500, random_state=1234, replacement=True, sampling_strategy=not minority; total time=   4.1s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=entropy, max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=500, random_state=1234, replacement=True, sampling_strategy=not minority; total time=   4.0s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=entropy, max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=500, random_state=1234, replacement=True, sampling_strategy=not minority; total time=   3.8s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=gini, max_depth=55, min_samples_leaf=4, min_samples_split=2, n_estimators=500, random_state=1234, replacement=True, sampling_strategy=all; total time=   3.3s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=gini, max_depth=55, min_samples_leaf=4, min_samples_split=2, n_estimators=500, random_state=1234, replacement=True, sampling_strategy=all; total time=   3.2s\n",
      "[CV] END bootstrap=False, class_weight=None, criterion=gini, max_depth=55, min_samples_leaf=4, min_samples_split=2, n_estimators=500, random_state=1234, replacement=True, sampling_strategy=all; total time=   3.1s\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "\n",
    "hyperparameter_dict = {\"n_estimators\": [100, 200, 500, 800, 1300, 1800, 2000, 2500],\n",
    "                       \"criterion\": [\"gini\", \"entropy\"],\n",
    "                       \"max_depth\": [None, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60],\n",
    "                       \"min_samples_split\": [2, 3, 4, 5, 7, 10, 15],\n",
    "                       \"min_samples_leaf\": [1, 2, 4, 5],\n",
    "                       \"bootstrap\": [False],\n",
    "                       \"sampling_strategy\": [\"majority\", \"not minority\", \"not majority\", \"all\"],\n",
    "                       \"replacement\": [True],\n",
    "                       \"random_state\": [1234],\n",
    "                       \"class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "                       }\n",
    "\n",
    "RandomSearchrf = imblearn.ensemble.BalancedRandomForestClassifier()\n",
    "\n",
    "rf_random = sklearn.model_selection.RandomizedSearchCV(estimator = RandomSearchrf, param_distributions = hyperparameter_dict, n_iter = 30, cv = 3, verbose=2, random_state=1234, scoring=\"recall_macro\")\n",
    "\n",
    "randsearch_output = rf_random.fit(refined_training, np.ravel(training_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see what our best classifier looked like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BalancedRandomForestClassifier(bootstrap=False, max_depth=40,\n",
       "                               min_samples_leaf=4, min_samples_split=3,\n",
       "                               n_estimators=500, random_state=1234,\n",
       "                               replacement=True,\n",
       "                               sampling_strategy='not minority')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randsearch_output.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and split our training data into training and validation, and compare a baseline classifier to our new classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zach/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/home/zach/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/home/zach/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/home/zach/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/home/zach/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/home/zach/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/home/zach/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/home/zach/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/home/zach/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/home/zach/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5970270424886259"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "baseline_classifier_2 = imblearn.ensemble.BalancedRandomForestClassifier(n_estimators=100, replacement=True, bootstrap=False, random_state=1234)\n",
    "cross_validator = sklearn.model_selection.KFold(n_splits=10, shuffle=True, random_state=1234)\n",
    "baseline_valid_results = sklearn.model_selection.cross_val_score(baseline_classifier_2, X=refined_training, y=np.ravel(training_y), groups=None, scoring=\"recall_macro\",cv=cross_validator)\n",
    "baseline_valid_results.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6007666205803488"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xvalidated_classifier = randsearch_output.best_estimator_\n",
    "\n",
    "optimal_model_results = sklearn.model_selection.cross_val_score(xvalidated_classifier, X=refined_training, y=np.ravel(training_y), groups=None, scoring=\"recall_macro\",cv=cross_validator)\n",
    "optimal_model_results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a macro average recall of 60% versus the 60% of the baseline classifier, we have a classifier that performs reasonably well overall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
