{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "\n",
    "Importing modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import imblearn\n",
    "import sklearn.inspection\n",
    "from itables import init_notebook_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in data from csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(\"manga.csv\")\n",
    "mod_date = pd.to_datetime(os.path.getmtime(\"manga.csv\"), unit=\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Exploration Phase 1**\n",
    "\n",
    "Showing the summary statistics for the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_notebook_mode(all_interactive=True)\n",
    "\n",
    "df_original.describe(include=\"all\").transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Wrangling**\n",
    "\n",
    "Removing columns that are not used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_1 = df_original.drop([\"id\",\"eng_title\",\"rom_title\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing values where the missingness actually means the value is effectively zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_2 = updated_df_1\n",
    "zeroed_columns = updated_df_2.select_dtypes(include=['number']).drop([\"start_year\",\"start_month\",\"start_day\",\"end_year\",\"end_month\",\"end_day\",\"chapters\",\"volumes\"], axis=1).columns\n",
    "updated_df_2[zeroed_columns] = updated_df_2[zeroed_columns].fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the missing rows for status manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.query(\"status.isnull()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.groupby(\"status\").agg({\"status\":\"count\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very likely that the first and third manga are releasing (have start dates but not end dates) and the second manga is completed (has both start and end dates, and cancelled manga are a small proportion of the data), so we will manually fill those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_3 = updated_df_2\n",
    "\n",
    "updated_df_3.loc[1272,\"status\"] = \"RELEASING\"\n",
    "updated_df_3.loc[2943,\"status\"] = \"FINISHED\"\n",
    "updated_df_3.loc[6862,\"status\"] = \"RELEASING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting status and country to one-hot encoded features in preparation for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_4 = pd.get_dummies(updated_df_3, dummy_na=False, columns=[\"status\",\"country\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a classifier to impute missing values for source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.groupby(\"source\").agg({\"source\":\"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_df_1 = updated_df_4.drop([\"chapters\",\"volumes\",\"start_month\",\"start_day\",\"end_year\",\"end_month\",\"end_day\"], axis=1)\n",
    "source_training = simple_df_1.dropna(subset=\"source\")\n",
    "\n",
    "# Using balanced random forest on account of highly imbalanced classes\n",
    "source_classifier = imblearn.ensemble.BalancedRandomForestClassifier(random_state=1234, sampling_strategy=\"all\",replacement=True,bootstrap=False, n_estimators=150)\n",
    "\n",
    "lightweight_cross_validator = sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "\n",
    "source_imputation_cv = sklearn.model_selection.cross_val_predict(estimator=source_classifier, X=source_training.drop(\"source\", axis=1), y=np.ravel(source_training[\"source\"]), cv=lightweight_cross_validator, method='predict')\n",
    "\n",
    "sklearn.metrics.ConfusionMatrixDisplay.from_predictions(np.ravel(source_training[\"source\"]), source_imputation_cv, labels=[\"LIGHT_NOVEL\",\"MANGA\",\"ORIGINAL\",\"OTHER\",\"VIDEO_GAME\",\"VISUAL_NOVEL\"],normalize=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_classifier.fit(X=source_training.drop(\"source\", axis=1), y=np.ravel(source_training[\"source\"]))\n",
    "\n",
    "source_preds = source_classifier.predict(updated_df_4.drop([\"chapters\",\"volumes\",\"start_month\",\"start_day\",\"end_year\",\"end_month\",\"end_day\",\"source\"], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_5 = updated_df_4\n",
    "updated_df_5 = updated_df_5.assign(imputed_source = source_preds)\n",
    "\n",
    "conditions_1 = [\n",
    "    updated_df_5[\"source\"].isna(),\n",
    "    updated_df_5[\"source\"].notna()\n",
    "]\n",
    "\n",
    "choices_1 = [\n",
    "    updated_df_5[\"imputed_source\"],\n",
    "    updated_df_5[\"source\"]\n",
    "]\n",
    "\n",
    "updated_df_5 = updated_df_5.assign(source = np.select(condlist=conditions_1, choicelist=choices_1, default=None) ).drop(\"imputed_source\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding one-hot encoding to the source feature as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_6 = pd.get_dummies(updated_df_5, dummy_na=False, columns=[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using stripped-down data frame as input for simple random forest to impute start years. Comparing cross-validated results with simply taking the median (the random forest performs better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_df_1 = updated_df_6.drop([\"chapters\",\"volumes\",\"start_month\",\"start_day\",\"end_year\",\"end_month\",\"end_day\"], axis=1)\n",
    "\n",
    "start_year_training = simple_df_1.dropna(subset=\"start_year\")\n",
    "\n",
    "start_year_regressor = sklearn.ensemble.RandomForestRegressor(n_estimators=100, random_state=1234, max_features = .15)\n",
    "\n",
    "baseline_results = sklearn.model_selection.cross_val_score(start_year_regressor, X=start_year_training.drop(\"start_year\", axis=1), y=np.ravel(start_year_training[\"start_year\"]), groups=None, scoring=\"neg_root_mean_squared_error\",cv=lightweight_cross_validator)\n",
    "baseline_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((start_year_training[\"start_year\"] - start_year_training[\"start_year\"].median())**2).mean()**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year_regressor.fit(start_year_training.drop(\"start_year\", axis=1), np.ravel(start_year_training[\"start_year\"]), )\n",
    "start_year_preds = start_year_regressor.predict(simple_df_1.drop(\"start_year\", axis=1)).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_7 = updated_df_6.assign(imputed_start_year = start_year_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_2 = [\n",
    "    pd.notna(updated_df_7[\"start_year\"]),\n",
    "    pd.isna(updated_df_7[\"start_year\"]) & pd.isna(updated_df_7[\"end_year\"]),\n",
    "    pd.isna(updated_df_7[\"start_year\"]) & updated_df_7[\"imputed_start_year\"] <= updated_df_7[\"end_year\"],\n",
    "    pd.isna(updated_df_7[\"start_year\"]) & updated_df_7[\"imputed_start_year\"] > updated_df_7[\"end_year\"]\n",
    "]\n",
    "\n",
    "choices_2 = [\n",
    "    updated_df_7[\"start_year\"],\n",
    "    updated_df_7[\"imputed_start_year\"],\n",
    "    updated_df_7[\"imputed_start_year\"],\n",
    "    updated_df_7[\"end_year\"]\n",
    "]\n",
    "\n",
    "updated_df_8 = updated_df_7.assign(start_year = np.select(condlist=conditions_2, choicelist=choices_2, default=None).astype(np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing start month using median value for each start year (using July when there are no non-missing values for that year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_month_imputation_key = updated_df_8.groupby(\"start_year\").agg({\"start_month\": \"median\"}).round().fillna(7)\n",
    "\n",
    "updated_df_9 = updated_df_8.join(other=start_month_imputation_key, on=\"start_year\", rsuffix=\"_imputed\")\n",
    "\n",
    "conditions_3 = [\n",
    "    pd.isna(updated_df_9[\"start_month\"]),\n",
    "    pd.notna(updated_df_9[\"start_month\"])\n",
    "]\n",
    "\n",
    "choices_3 = [\n",
    "    updated_df_9[\"start_month_imputed\"],\n",
    "    updated_df_9[\"start_month\"]\n",
    "]\n",
    "\n",
    "updated_df_9 = updated_df_9.assign(start_month = np.select(condlist =conditions_3, choicelist=choices_3, default=None) ).astype(np.float64).drop(\"start_month_imputed\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing start day using median value for each start year and month combination (using the 14th when there are no non-missing values for that combination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_day_imputation_key = updated_df_9.groupby([\"start_year\",\"start_month\"]).agg({\"start_day\": \"median\"}).round().fillna(14)\n",
    "\n",
    "updated_df_10 = updated_df_9.join(other=start_day_imputation_key, on=[\"start_year\",\"start_month\"], rsuffix=\"_imputed\")\n",
    "\n",
    "conditions_4 = [\n",
    "    pd.isna(updated_df_10[\"start_day\"]),\n",
    "    pd.notna(updated_df_10[\"start_day\"])\n",
    "]\n",
    "\n",
    "choices_4 = [\n",
    "    updated_df_10[\"start_day_imputed\"],\n",
    "    updated_df_10[\"start_day\"]\n",
    "]\n",
    "\n",
    "updated_df_10 = updated_df_10.assign(start_day = np.select(condlist =conditions_4, choicelist=choices_4, default=None) ).astype(np.float64).drop(\"start_day_imputed\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute some end dates by assigning most recent data-gathering date to those currently running with missing end date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_5 = [\n",
    "    pd.isna(updated_df_10[\"end_year\"]) & pd.isna(updated_df_10[\"chapters\"]) & pd.isna(updated_df_10[\"volumes\"]) & updated_df_10[\"status_RELEASING\"] == 1,\n",
    "    updated_df_10[\"status_RELEASING\"] != 1 | pd.notna(updated_df_10[\"end_year\"]) | pd.notna(updated_df_10[\"chapters\"]) | pd.notna(updated_df_10[\"volumes\"])\n",
    "]\n",
    "\n",
    "choices_5_1 = [\n",
    "    mod_date.day,\n",
    "    updated_df_10[\"end_day\"]\n",
    "]\n",
    "\n",
    "choices_5_2 = [\n",
    "    mod_date.month,\n",
    "    updated_df_10[\"end_month\"]\n",
    "]\n",
    "\n",
    "choices_5_3 = [\n",
    "    mod_date.year,\n",
    "    updated_df_10[\"end_year\"]\n",
    "]\n",
    "\n",
    "updated_df_11 = updated_df_10.assign(end_day = np.select(condlist =conditions_5, choicelist=choices_5_1, default=None).astype(np.float64)).assign(end_month = np.select(condlist =conditions_5, choicelist=choices_5_2, default=None).astype(np.float64)).assign(end_year = np.select(condlist =conditions_5, choicelist=choices_5_3, default=None).astype(np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing end month from median of each year, where there is a non-missing end year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_month_imputation_key = updated_df_11.groupby(\"end_year\").agg({\"end_month\": \"median\"}).round()\n",
    "\n",
    "updated_df_12 = updated_df_11.join(other=end_month_imputation_key, on=\"end_year\", rsuffix=\"_imputed\")\n",
    "\n",
    "conditions_6 = [\n",
    "    pd.isna(updated_df_12[\"end_month\"]),\n",
    "    pd.notna(updated_df_12[\"end_month\"])\n",
    "]\n",
    "\n",
    "choices_6 = [\n",
    "    updated_df_12[\"end_month_imputed\"],\n",
    "    updated_df_12[\"end_month\"]\n",
    "]\n",
    "\n",
    "updated_df_12 = updated_df_12.assign(end_month = np.select(condlist =conditions_6, choicelist=choices_6, default=None) ).astype(np.float64).drop(\"end_month_imputed\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing end day from median of each year and month combination where these values are both non-missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_day_imputation_key = updated_df_12.groupby([\"end_year\",\"end_month\"]).agg({\"end_day\": \"median\"}).round()\n",
    "\n",
    "updated_df_13 = updated_df_12.join(other=end_day_imputation_key, on=[\"end_year\",\"end_month\"], rsuffix=\"_imputed\")\n",
    "\n",
    "conditions_7 = [\n",
    "    pd.isna(updated_df_13[\"end_day\"]),\n",
    "    pd.notna(updated_df_13[\"end_day\"])\n",
    "]\n",
    "\n",
    "choices_7 = [\n",
    "    updated_df_13[\"end_day_imputed\"],\n",
    "    updated_df_13[\"end_day\"]\n",
    "]\n",
    "\n",
    "updated_df_13 = updated_df_13.assign(start_day = np.select(condlist =conditions_7, choicelist=choices_7, default=None) ).astype(np.float64).drop(\"end_day_imputed\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create start and end date columns that combine day/month/year into single data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df_14 = updated_df_13.assign(start_date = pd.to_datetime(arg=updated_df_11[[\"start_year\",\"start_month\",\"start_day\"]].rename(columns={\"start_year\": \"year\", \"start_month\": \"month\", \"start_day\": \"day\"}), errors=\"coerce\"),\n",
    "                                      end_date = pd.to_datetime(arg=updated_df_11[[\"end_year\",\"end_month\",\"end_day\"]].rename(columns={\"end_year\": \"year\", \"end_month\": \"month\", \"end_day\": \"day\"}), errors=\"coerce\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
